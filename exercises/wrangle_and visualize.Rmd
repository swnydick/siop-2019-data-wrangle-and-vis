---
title: 'Data: Wrangle & Display It With (Relative) Ease'
author: "Korn Ferry Institute"
date: ''
output:
  html_document:
    df_print: paged
  html_notebook:
    highlight: zenburn
    theme: spacelab
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(out.width='1600px', out.height = '900px', dpi=200)
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```

</br> 

# Data Cleaning

In this tutorial, we will be taking in two data sets from a fictional company, 
one being data on the employees' dempographics and the other is some fictional test scores.
We will be cleaning and reformatting them to make them usable, 
joining them together, and producing some graphics from them. 
Happily, all of this is actually pretty easy in R.

</br>

## Dependencies and setup

Make sure you have the following packages installed
* `openxlsx`
* `reshape2`
* `dplyr`
* `magrittr`
* `ggplot2`
* `ggthemes`
* `roperators`



I've attached an installation script that you can run to make sure the packages are all there:

```{r eval = FALSE}
pkgs <- c("openxlsx", "reshape2", "dplyr", "magrittr", 
          "ggplot2",  "ggthemes", "roperators")

# Hackfix tip: 
# Opening the CRAN mirror in a browser can help with some restricted networks
utils::browseURL("http://cran.stat.auckland.ac.nz/")
utils::browseURL("https://meta.wikimedia.org/wiki/List_of_countries_by_regional_classification")

# Another hackfix for restricted networks.... just in case
httr::set_config(httr::config( ssl_verifypeer = 0L ))

for (package in pkgs){
  if(package %in% rownames(installed.packages()) == FALSE)
    # Try will attempt to do what is in parentheses, but won't die if it doesn't work
    try(install.packages(package, repos = "http://cran.stat.auckland.ac.nz/"), silent = TRUE)
}

```

 </br>
 
### Load your required packages 

```{r libs, message=FALSE}
require(openxlsx)
require(reshape2)
require(dplyr)
require(magrittr)
require(ggplot2)
require(ggthemes)
require(roperators)

```
</br>

### Load The Data

In our data folder, there are two datasets, a csv called employee_data.csv and 
an excel workbook called survey_results.xlsx


To read in the csv data, we can use base R's `read.csv` function, which is the same as
`read.table`, which you might see in other scripts, only with different default arguments. 
What's nice aboput data stored in csv files is that because they're just plain-text
flatfiles, they can be opened in any program or programming language. 



**The following code reads:**


* Create a variable called employees


* Into that value place the output of `read.csv()`


* ...Where `read.csv()` is going to go out one folder (../) and then look for `employee_data.csv` in `data`


* And finally, don't turn text variables into factors automatically! 

```{r read_csv}
employees <- read.csv("../data/employee_data.csv", stringsAsFactors = FALSE)
```


Excel's files are a little trickier to read in which is why we loaded the `openxlsx` 
package to handle it. There are other, older packages to read in .xlsx doccuments, 
however, they often have difficult Java dependencies, hence we prefer `openxlsx`

```{r read_xlsx}
survey <- read.xlsx("../data/survey_results.xlsx")
```



## Inspect The Data

Now that our data has been read into R as `dataframe`s (which are just glorified 
lists of vectors that are all the same length), we can inspect it. If you're in 
`RStudio` you can go to the environment pane (usually top-right) and click on the
names of either dataframe to open a preview of it. 

We can also take a quick look at the data programatically by looking at the `head`
(first 6 rows) like so:

```{r call_head}
head(employees)

```

**Note** that the age column has a ".." where there's a missing value (normally `NA`
in R) and the hiring date `h_date` is in a non-standard format (data people like
dates to be in `yyyy-mm-dd` format). We'll circle back to fix those issues later. 


Now, if we look at the `survey` data, we can see something troubling...
```{r call_head_survey, output.lines=6}
head(survey)
```


**Yeesh** it looks like some well-meaning but not-data-minded person went and 
stored the survey results in wide format with one column per respondant and one
row per question. 


Before we can work with this data, **We have the following problems to solve:**

1) Missing values are coded as ".." which has turned all of the numeric columns into text!
2) Dates need to be transformed before they can be used
3) Values from each person are placed in different columns
4) Variables are seperated by row, not column meaning in each row are from different variables hence the yearly column statistics are meaningless


## Fix The Data

### Fix missing values
Firstly, let's replace those ".." missing value codes with missing values (`NA`). 
We also want to convert the age column into a `numeric` type. 
Think about the logic we want to use: we want to go through the age columns of `employees` 
turn all cells with `..` into `NA`, and then convert the columns into `numeric`.

Happily, this is all rather easy, 

**Note** In some cases, such as using `gsub()` text that contains dots, such as our `..`s will requie a _regular expression_. _Regular expressions_ are a tricky subject, basically they're specially formatted text used to select specific formats of text. You can use _regular expression_`s to do some very complicated text manipulation - but it's more efficient to just look on <http://stackoverflow.com/questions/tagged/regex> for an existing example of something similar to what you're wanting to do.

Here, we'll use simple **logical indexing** to find and replace all cells that have ".." with missing values (`NA` in R)

**The following code reads:**


* Take the `age` column from employees (accessed with `$`)

* In the age column, find all rows where age is `".."` 

* Replace those entries with a missing value, `NA`

* Turn the age column into a `numeric` colummn


```{r fix_na_gsub}
employees$age[employees$age == ".."] <- NA
employees$age                        <- as.numeric(employees$age)
## Read as: 
## employee$age[where employees$age is equal to ".."] replace with NA
## ....but we want to teach you new tricks!
```

### Fix dates

The hiring date column (`employees$h_date`) is currently a `character` string
AKA a text variable. We'll want to make that into a proper `datetime` variable first
so that we can do interesting stuff, like work out how many years someone has been at the 
company. To do this, we'll use the base R date functions. Note that you can work with dates
in in packages like `lubridate` which just make some things a little more straightforward :)

In the following code, we need to tell R what formt the date is currently in!

The code reads:


* Overwrite the employee_data$h_date column with the output of `as.Date()`


* Within `as.Date()`, transform the existing `h_date` column


* ...And read the text as numeric month (%m) / numeric day (%d) / 4 digit year (%Y)


**Hint:** to see all available date formatting options, run `?strptime` in the console

```{r}
employees$h_date <- as.Date(employees$h_date, format = "%m/%d/%Y")
```


Now, lets work out how long they've worked here for

Read the following code as:

* Create a column in `employee_data` called `tenure`

* into that column put the result of:

* today's date (`Sys.Date()`) minus the date people were hired (turned into `numeric` from a `difftime` class)

* ...then divide that by ~365.25 days to give their tenure in years 

```{r}
employees$tenure <- as.numeric(Sys.Date() - employees$h_date)/365.2422
```

And now let's put people's tenures into categories, let's do: 0-1 year, 1-2 years,
2-5 years, 5-10 years, and 10+ years.

To accomplish this, we'll user R's `cut()` function to "cut" out numeric veriable into 
categories. 

`cut()` is using these arguments:

* the variable being transformed

* the break points (note `0` and `Inf` on the ends)

* the labels corresponding to the intervals between the breaks

* a logical flg telling it whether the breaks are including the number on the right 
of the interval. If `FALSE` it's 0 to less than 1, 2 to less than 3; if `TRUE` it's
0 to 1, over 1 to 2, over 2 to 3, etc. 

* another flag telling R that you'd like the resulting factor levels to be ordered the way you specified it. 

```{r}
employees$tenure_label <- cut(employees$tenure,
                              breaks = c(0, 1, 2, 5, 10, Inf),
                              labels = c("<1 year", "1-2 years", 
                                         "2-5 years", "5-10 years", 
                                         "10+ years"),
                              right = FALSE,
                              ordered_result = TRUE)

```

#### Inspect the data

Now checkout the distribution of the variables we just created! We'll just use 
quick-and-dirty plots for initial visualizations
```{r}
# A histogram of employee age
# hist(employees$age) # if you want to look at the distribution of employee ages
plot(employees$tenure_label)
```


### Reshape the scores data

Things are looking promising! But we still have to do something about the scores dataframe.
It's be nice to be able to break down our scores by employee demographics, however, that isn't so straight-forward given that the scores are in another dataframe that is in wide format with one column per candidate. Happily, that won't take us long to fix! We'll use a package called `reshape2` to turn the datafrme into a nice long format (one row per candidate and one column per question/variable).


First, let's `melt` the data into a long format (think of cheese melting off of a delicious pizza if it makes the function name more intuitive).  To use `melt()` we need to specify the following:

* the dataframe we want to transform

* the id variable we want the 'melted' data mapped to

* the `variable.name` is the name of the column where the names of the melted columns will go

* the `value.name` is the name of the column where the values of the melted columns will go

```{r}
survey <- melt(survey, 
               id.vars = "question", 
               variable.name = "id", 
               value.name = "score") 

```

Now, let's take a peek at what `survey` looks like now:

```{r}
head(survey)
```

Hmm, it looks like we're getting there, however now the data format is a bit too long to be practical since the scores from each question are all in the same column. Let's widen it up just a little bit to get the scores from each question into their own columns. To do this we use `dcast()` to **cast** the **d**ata (think of casting a fishing line). 

To use `dcast()` we need to specify:

* the data we want to transform

* a formula that looks like: `<row for observations> ~ <column to seperate out>`

```{r}
survey <- dcast(survey, id ~ question)
```

Now, let's take a look-see...

```{r}
head(survey)
```

Perfect! Now we have one row per observation, one column per variable! Next up, let's join the scores to the employee data by their ID number. 


## Join the dataframes together

Alright, now we just need to join our dataframes, `employees` and `survey` together. 
While you could force them together by sorting by id and using `cbind()` to effectivelly slap the columns together, that isn't really a good idea in most real-world situations as the employee id numbers may not be perfectly alligned due to discrepencies between people in the employee database and people who took the survey. That's why we'll do a formal `join` on the dataframes. 

If you've used `SQL` to query and manipulate data or `VLOOKUP` in Excel, this may be a familiar concept. Put simply, we want to find rows where the id varible matches between our two dataframes and copy over the corresponding survey results. In this case, since we're appending survey results onto the employee data, we're technically going to do a `left outer` join - i.e. keep all employee data on the laft hand side and add survey results (on the right-hand side) if we have them. 

The main types of join you'll see used are:

* left outer (keep everything on the left, add data from the right if there's a match)

* right outer (keep everything on the right, add data from the left if there's a match)

* inner (only keep rows where there's a match between the left and right tables)

* outer (keep all data and fill non-matched records with `NA`s)

Within R there are three main ways to join data: 

* `merge()` in base R - it works, but the syntax is a little gross

* `data.table` join by reference - lightning fast and great for big data but the syntax isn't intuitive

* `_join` functions from `dplyr` - easy to use, albeit not nearly as fast as `data.table`

Here, we'll use left_join from dplyr with the following logic:

* Make sure the id variables in both dataframes are the same type - `character` is a good thing to convert them to.

* Call `left_join()` and supply the left and right data tables (employees and survey respectivelly), and by as a named character vector in the form of `c(left_id_column = right_id_column)


```{r}
employees$employee_id <- as.character(employees$employee_id)
survey$id             <- as.character(survey$id)
all_data              <- left_join(employees, survey, 
                                   by = c("employee_id" = "id"))
head(all_data)
```

Hooray! Now that our data is all pretty like, we can do some aggregations! 



## Make basic aggregations


***`filter`**    - Get a subset of rows

***`select`**    - Get a subset of columns

***`group_by`**  - Tag data for grouped calculations

***`summarise`**  - Create aggregated data summaries and apply functions to data

***`mutate`**    - Add a new variable (also works with grouped data)

***`rename`**    - Rename variables

***`arrange`**   - Sort the data by selected columns

***`do`**        - Do an arbitrary thing




## Use pipes to make easy aggregations

## Make advanced aggregations

## Put aggregations into visuals
